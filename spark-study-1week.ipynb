{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNl0Hz9/XFlmRQQXnLIOpeh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 아파치 스파크란\n","통합 컴퓨팅 엔진이며 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합이다.\n","\n","스파크에서 제공하는 전체 컴포넌트와 라이브러리는 다음과 같다\n","1. 구조적 스트리밍, 고급 분석, 라이브러리 및 에코시스템\n","2. 구조적 API(Dataset, DataFrame, SQL)\n","3. 저수준 API(RDD, 분산형 변수)\n","\n"],"metadata":{"id":"x5xdxbbIszyn"}},{"cell_type":"markdown","source":["##**컴퓨팅 엔진**\n","\n","스파크는 저장소 시스템의 데이터를 연산하는 역할만 수행할 뿐.\n","영구 저장소 역할은 수행하지 않는다.\n","\n","그 대신 클라우드 기반의 애저 스토리지, 아마존 S3, 아파치 하둡, 아파치 키산드라, 메세지 전달 서비스인 아파치 카프카등의 저장소를 지원.\n","\n","##**하둡 맵리듀스**\n","\n","하둡 맵리듀스는 수천 개의 노드로 구성된 클러스터에서 병렬로 데이터를 처리하는\n","\n","##**첫번쨰 기능**\n","배치 애플리케이션\n","\n","## **두번쨰 기능**\n","대화영 데이터 분석\n","\n","## **세번쨰 기능**\n","비정형 쿼리\n","\n","##**스파크의 초기버전**\n","함수형 연산(자바 객체로 이루어진 컬렉션에 맵이나 리듀스 같은 병렬 연산을 수행하는 방식) 관점에서 API를 정의\n","\n","##**1.0 버전**\n","구조화된 데이터(?)를 기반으로 동작하는 신규 API인 SQL이 추가됨.\n","(고정형 데이터 포멧을 사용하는 스파크 테이블은 자바의 인메모리 데이터 표현 방식에 종속되지 않는다.)\n","\n","##**그 이후 **\n","DataFrame, 머신러닝, 파이프라인, 자동 최적화를 수행하는 구조적 스트리밍 등\n","\n","\n"],"metadata":{"id":"GK--4axhs6lq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jm03lZ9_reaK"},"outputs":[],"source":["# !pip install pyspark\n","# !pip install -U -q PyDrive\n","# !apt install openjdk-8-jdk-headless -qq\n","# import os\n","# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""]},{"cell_type":"code","source":["import pyspark\n","import pyspark.sql as pyspark_sql\n","import pyspark.sql.types as pyspark_types\n","import pyspark.sql.functions as pyspark_functions\n","from pyspark import SparkContext, SparkConf"],"metadata":{"id":"hx14__OBr2b6","executionInfo":{"status":"ok","timestamp":1708860193515,"user_tz":-540,"elapsed":2,"user":{"displayName":"YC Choi","userId":"11699591210195301986"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# create the session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","\n","# create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = pyspark_sql.SparkSession.builder.getOrCreate()"],"metadata":{"id":"cj-0ccElrtmd","executionInfo":{"status":"ok","timestamp":1708860211948,"user_tz":-540,"elapsed":16207,"user":{"displayName":"YC Choi","userId":"11699591210195301986"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g5LlcBEJsTza"},"execution_count":null,"outputs":[]}]}