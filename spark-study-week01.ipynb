{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3d86680-b806-4b88-980a-b3d3a86b739a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Apache Spark?\n",
    "- 빅데이터 처리를 위한 통합 컴퓨팅 엔진, 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합\n",
    "- -> 사전에, 실제 연산을 수행하는 컴퓨팅 노드를 추가/삭제를 쉽게 할 수 있음\n",
    "\n",
    "여기서 `통합`, `컴퓨팅 엔진`과 `라이브러리 집합`이 언급됐는데, 이들은 무엇인가?\n",
    "\n",
    "## Aparch spark concept\n",
    "\n",
    "### 통합\n",
    "- 통합은 우리가 무슨 작업(e.g. read, sql연산, ML, stream 처리)을 하든, 동일한 연산엔진과 일관성있는 api를 제공한다는 것을 의미\n",
    "- -> 따라서, sql 연산을 하든, stream 처리를 하든 우리가 사용하는 api는 크게 다르지 않음\n",
    "- spark가 나오기 전에는 시스템에서 제공해주는 라이브러리와 다른 소프트웨어에서 제공하는 api를 결합해서 사용해서 데이터를 병렬 처리 하였음\n",
    "\n",
    "### 컴퓨팅 엔진\n",
    "- spark는 데이터에 대한 연산만 수행 할 뿐이다.\n",
    "- -> read/write를 하기 위해서, 우리는 별도의 저장소를 따로 준비해야한다.\n",
    "- 대신, spark는 여러 저장소에서 읽고 쓸 수 있는 것을 지원한다.(e.g. monogodb connector for spark, spark-es)\n",
    "- -> 해당 저장소가 spark에서 읽고 쓸 수 있는지, 확인하기 위해서는 해당 저장소가 spark를 지원하는지 찾아봐야함\n",
    "- 사용하는 저장소에 상관없이, 우리는 일관성 있는 api를 사용하기 때문에, `SparkConf`, `format`, `option` 부분의 인수만 저장소 스펙에 맞게 변경하면 된다.\n",
    "\n",
    "**spark read from es**\n",
    "```java\n",
    "SparkConf conf = new SparkConf();\n",
    "conf.set(\"spark.es.nodes\", \"your host name\");\n",
    "conf.set(\"spark.es.port\", \"your port number\");\n",
    "\n",
    "SparkSession sparkSession = SparkSession.builder().config(conf).getOrCreate();\n",
    "DataSet<Row> dataset = sparkSession.read().format(\"org.elasticsearch.spark.sql\").option(\"es.resource\", \"your index with type\").load()\n",
    "```\n",
    "\n",
    "**spark read from mongodb**\n",
    "```java\n",
    "SparkConf conf = new SparkConf();\n",
    "conf.set(\"spark.mongodb.read.connection.uri\", \"your endpoint\");\n",
    "SparkSession sparkSession = SparkSession().builder().config(conf).getOrCreate();\n",
    "DataSet<Row> dataset = sparkSession().read().foramt(\"mongodb\").option(\"spark.mongodb.read.database\", \"your db\").option(\"spark.mongodb.read.collection\", \"your collection\").load()\n",
    "```\n",
    "\n",
    "더 나아가, spark는 한 저장소에서 읽어서 다른 저장소에 저장하는 것을 지원한다.(e.g. mongodb로부터 읽어서, es에 write하기)\n",
    "\n",
    "### 라이브러리\n",
    "- spark는 표준 라이브러이와, 외부 라이브러리를 제공한다.\n",
    "- -> 위의 언급된 mongodb connector for spark, spark-es는 spark에서 만든 라이브러리는 아니며, mongodb, es에서 만들었음\n",
    "- spark에서 제공하는 라이브러리로는 spark sql, graphx가 있음\n",
    "\n",
    "## Spark가 세상에 나온 이유\n",
    "- CPU 발전의 한계로 인해, 병렬 컴퓨팅의 중요성\n",
    "  - 특히 데이터 수집 비용이 저렴해지면서, 데이터를 많이 얻을 수 있지만, 이를 처리하는 환경은 여전히 부족했다.\n",
    "\n",
    "## Spark history\n",
    "기존에는 데이터를 병렬처리하기 위해 hadoop MR을 사용했었는데, 단점으로는 다음과 같았음\n",
    "1. 각 단계별로 MR job을 개발해서 각 job을 클러스터에서 개별적으로 실행해야하는 비효율성\n",
    "- iterative processing에 적합하지 않음\n",
    "2. map과 reduce phase 사이에서, disk에 read/wrtie 연산 발생\n",
    "\n",
    "이 문제를 해결하기 위해 \n",
    "- spark는 여러 단계로 이루어진 애플리케이션을 쉽게 개발 할 수 있도록 함수형 기반의 api로 설계\n",
    "- 연산 단계 사이에 데이터를 메모리에 저장함으로써 효율성 증가\n",
    "\n",
    "초기에는 함수형 기반의 api에 중점을 뒀으나, 이후 버전 1.0 이후 구조화된 데이터를 기반으로 동작하는 신규 spark sql 추가\n",
    "\n",
    "## Spark 실습\n",
    "- 데이터 브릭스에서 제공하는 커뮤니티 사용했음.\n",
    "- -> jupyter notebook에서 테스트하면서 배운 내용을 정리하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e72ebfb-edd0-4221-8503-c5283cc55ad1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb9cdf9-1edb-4197-b20d-4e47911b1f52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession.builder.appName(\"Hello world\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return sc\n",
    "\n",
    "def main():\n",
    "    sc = init_spark()\n",
    "    nums = sc.parallelize([1,2,3,4])\n",
    "    print(nums.map(lambda x: x**2).collect())\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86380c66-5b8b-4946-b7a1-30ac5cd27402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2484833-63d5-47bc-8f45-5e3f8720cd2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08dbf4d6-7200-4ebf-91ed-01ab3ef26051",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark test",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
